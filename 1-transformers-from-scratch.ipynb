{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers desde (casi) cero\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ohtar10/icesi-nlp/blob/main/Sesion3/1-transformers-from-scratch.ipynb)\n",
    "\n",
    "En este notebook implementaremos un clasificador de noticias en español utilizando transformers. Implementaremos parte de la arquitectura del modelo pieza por pieza para ver como funciona por dentro. Sin embargo, utilizarémos las utilidades de tokenización de huggingface transformers para ayudarnos con esta tarea.\n",
    "\n",
    "#### Referencias\n",
    "- Dataset: https://huggingface.co/datasets/MarcOrfilaCarreras/spanish-news\n",
    "- [Attention is All You Need](http://arxiv.org/abs/1706.03762)\n",
    "- [Natural Language Processing with Transformers: Building Language Applications With Hugging Face](https://www.amazon.com/Natural-Language-Processing-Transformers-Applications/dp/1098103246)\n",
    "- [Tutorial 5: Transformers and Multi-Head Attention](https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/05-transformers-and-MH-attention.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "installed_packages = [package.key for package in pkg_resources.working_set]\n",
    "IN_COLAB = 'google-colab' in installed_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!test '{IN_COLAB}' = 'True' && wget  https://github.com/Ohtar10/icesi-nlp/raw/refs/heads/main/requirements.txt && pip install -r requirements.txt\n",
    "!test '{IN_COLAB}' = 'True' && sudo apt-get update -y\n",
    "!test '{IN_COLAB}' = 'True' && sudo apt-get install python3.10 python3.10-distutils python3.10-lib2to3 -y\n",
    "!test '{IN_COLAB}' = 'True' && sudo update-alternatives --install /usr/local/bin/python python /usr/bin/python3.11 2\n",
    "!test '{IN_COLAB}' = 'True' && sudo update-alternatives --install /usr/local/bin/python python /usr/bin/python3.10 1\n",
    "!test '{IN_COLAB}' = 'True' && pip install lightning datasets 'transformers[torch]' sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargando el dataset\n",
    "Este es un dataset pequeño de articulos de noticias en idioma español con sus respectivas categorías. El dataset está disponible en el HuggingFace Hub y puede ser fácilmente descargado con la librería."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/anaconda3/envs/nlp_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "dataset = load_dataset(\"wikiann\", \"es\", split='train')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET wikiANN (es)\n",
    "\n",
    "#### **Columnas principales**\n",
    "\n",
    "* `tokens`: Lista de palabras (ej. `[\"José\", \"Luis\", \"García\"]`).\n",
    "* `ner_tags`: Lista de enteros que representan etiquetas IOB.\n",
    "* `langs`: Idioma por token (aquí siempre `\"es\"`).\n",
    "* `spans`: Entidades completas (ej. `\"PER: José Luis García\"`).\n",
    "\n",
    "#### **Formato de Etiquetas (IOB)**\n",
    "\n",
    "El dataset usa un esquema IOB con 3 tipos de entidades: **Persona (PER)**, **Organización (ORG)** y **Lugar (LOC)**.\n",
    "\n",
    "| ID | Tag   | Significado               |\n",
    "| -- | ----- | ------------------------- |\n",
    "| 0  | O     | Outside (ninguna entidad) |\n",
    "| 1  | B-PER | Beginning of Person       |\n",
    "| 2  | I-PER | Inside Person             |\n",
    "| 3  | B-ORG | Beginning of Organization |\n",
    "| 4  | I-ORG | Inside Organization       |\n",
    "| 5  | B-LOC | Beginning of Location     |\n",
    "| 6  | I-LOC | Inside Location           |\n",
    "\n",
    "**Número de etiquetas (`num_labels`) = 7**\n",
    "\n",
    "#### **Ejemplo de muestra**\n",
    "\n",
    "```python\n",
    "{\n",
    " 'tokens': ['REDIRECCIÓN', 'José', 'Luis', 'García'],\n",
    " 'ner_tags': [0, 1, 2, 2],\n",
    " 'langs': ['es', 'es', 'es', 'es'],\n",
    " 'spans': ['PER: José Luis García']\n",
    "}\n",
    "```\n",
    "\n",
    "Interpretación:\n",
    "\n",
    "```\n",
    "'REDIRECCIÓN' → O\n",
    "'José'        → B-PER\n",
    "'Luis'        → I-PER\n",
    "'García'      → I-PER\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Condado', 'de', 'Duplin', 'suroeste'],\n",
       " 'ner_tags': [5, 6, 6, 0],\n",
       " 'langs': ['es', 'es', 'es', 'es'],\n",
       " 'spans': ['LOC: Condado de Duplin']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los efectos de esta tarea, nos servirán el texto y la categoría naturalmente.\n",
    "\n",
    "A manera general, observemos que tan largos o cortos tienden a ser los textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto más corto: 3\n",
      "Texto más largo: 64\n",
      "Longitud promedio: 6.46415\n"
     ]
    }
   ],
   "source": [
    "text_lengths = [len(row['tokens']) for row in dataset]\n",
    "print(f\"Texto más corto: {min(text_lengths)}\")\n",
    "print(f\"Texto más largo: {max(text_lengths)}\")\n",
    "print(f\"Longitud promedio: {sum(text_lengths) / len(text_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos valores son la cantidad de *caractéres* que tiene las secuencias. Una decisión ingenua pero útil en este momento podría ser ajustar la longitud de las secuencias que vamos a usar para el entrenamiento a unos 2000 tokens. Esto podría ser suficiente para capturar una porción significativa de los textos.\n",
    "\n",
    "## Definiendo el Tokenizer\n",
    "\n",
    "Ahora, vamos a definir el tokenizer para nuestra tarea. Para ahorrarnos tiempo, vamos a entrenar uno basado en gpt2, pero ajustandolo a nuestro dataset. Para ello, debemos seleccionar una muestra representativa de nuestro dataset, como no es muy grande, lo usare todo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3662.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "\n",
    "length = 10000\n",
    "iter_dataset = iter(dataset)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "byte_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_byte_map.keys())\n",
    "\n",
    "def batch_iterator(batch_size: int = 10):\n",
    "    for _ in tqdm(range(0, length, batch_size)):        \n",
    "        examples = [next(iter_dataset) for _ in range(batch_size)]\n",
    "        yield [\" \".join(ex['tokens']) for ex in examples]\n",
    "        #yield [next(iter_dataset)['text'] for _ in range(batch_size)]\n",
    "\n",
    "\n",
    "ner_tokenizer = tokenizer.train_new_from_iterator(\n",
    "    batch_iterator(),\n",
    "    vocab_size=20000,            # tamaño más razonable para tu GPU\n",
    "    initial_alphabet=base_vocab\n",
    ")\n",
    "#spanish_news_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=50000, initial_alphabet=base_vocab)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3819.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "✅ New tokenizer trained!\n",
      "Vocab size: 20000\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "# Use GPT-2 tokenizer as base, enabling prefix space for pre-tokenized data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", add_prefix_space=True)\n",
    "\n",
    "# Build initial alphabet from GPT-2 byte-level BPE\n",
    "byte_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_byte_map.keys())\n",
    "\n",
    "# Iterator for WikiANN dataset\n",
    "length = 10000  # or len(dataset) if you want the full dataset\n",
    "iter_dataset = iter(dataset)\n",
    "\n",
    "def batch_iterator(batch_size: int = 10):\n",
    "    for _ in tqdm(range(0, length, batch_size)):\n",
    "        examples = [next(iter_dataset) for _ in range(batch_size)]\n",
    "        yield [\" \".join(ex['tokens']) for ex in examples]  # ✅ FIX\n",
    "\n",
    "# Train new tokenizer from raw text (joined tokens)\n",
    "ner_tokenizer = tokenizer.train_new_from_iterator(\n",
    "    batch_iterator(),\n",
    "    vocab_size=20000,            # Adjust based on GPU/memory\n",
    "    initial_alphabet=base_vocab\n",
    ")\n",
    "\n",
    "print(\"✅ New tokenizer trained!\")\n",
    "print(\"Vocab size:\", ner_tokenizer.vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploremos ahora el tokenizador obtenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: 20000 tokens\n",
      "Primeros 15 tokens:\n",
      "['<|endoftext|>', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.']\n",
      "15 tokens de en medio:\n",
      "['él', 'rig', ' He', 'tag', ' Unión', ' 27', 'ack', ' Social', '199', ' junto', ' Ángel', 'bu', 'bal', 'par', 'pol']\n",
      "Últimos 15 tokens:\n",
      "['inite', ' Pinilla', ' Pineda', ' Pinares', ' Pinotti', ' Pinatar', ' Pincheira', ' Bose', ' Bosco', ' Bossoni', ' Brain', ' Brave', ' Bracho', ' Brawn', ' Brabham']\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(ner_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)\n",
    "print(f\"Vocabulario: {ner_tokenizer.vocab_size} tokens\")\n",
    "print(\"Primeros 15 tokens:\")\n",
    "print([f\"{ner_tokenizer.convert_tokens_to_string([t])}\" for t, _ in tokens[:15]])\n",
    "print(\"15 tokens de en medio:\")\n",
    "print([f\"{ner_tokenizer.convert_tokens_to_string([t])}\" for t, _ in tokens[1000:1015]])\n",
    "print(\"Últimos 15 tokens:\")\n",
    "print([f\"{ner_tokenizer.convert_tokens_to_string([t])}\" for t, _ in tokens[-15:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que los primeros tokens corresponden a caracteres especiales y puntiación. Luego en el medio tenemos una combinación entre palabras completas y cortadas, el tokenizador se encarga de encontrar las frecuencias más comunes y asi partir las palabras por aquellas partes que tienden a repetirse mas. Esto es muy útil para trabajar con modelos de lenguaje ya que el modelo se vuelve robusto a diferentes ramificaciones de palabras e incluso a errores de tipografía. Finalmente, al final, vemos que tenemos más palabras cortadas y palabras muy especiales. Algo importante aquí es que podamos ver que los tokens tienen sentido con respecto a nuestro corpus.\n",
    "\n",
    "Ahora veamos como convierte el tokenizador una oración muy sencilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [497, 1548, 4560, 1, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tokenizer.pad_token = '[PAD]'\n",
    "ner_tokenizer(\"hola mundo!\", max_length=8, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que obtenemos de vuelta son los ids de cada token según el vocabulario. Ahora algo importante que notamos aquí es el *padding*, durante el entrenamiento, queremos que las secuencias sean de tamaño fijo, para asi operar comodamente con matrices. Pero ya vimos que no todos los textos tienen la misma longitud. Entonces que hacer? para los que son más largos que una longitud dada simplemente cortamos, pero para los que son más cortos, debemos *rellenar* lo faltante con un *token especial de relleno o padding*. Y es justo lo que definimos allí, cuando la cadena es inferior a 8 **tokens**, entonces debemos hacer padding hasta que se cumplan los 8.\n",
    "\n",
    "Ahora, notemos que \"hola mundo!\" son 2 palabras, 9 letras, 1 espacio y 1 simbolo para un total de 11 caracteres, pero vemos que el resultado son 4 tokens y el padding. Esto es trabajo del tokenizador. Cuando lo entrenamos con nuestro corpus, el tokenizador computó las frecuencias de palabras y sus partes, tal como vimos arriba, entonces, estos tokens juntos forman la frase original, observemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġh', 'ola', 'Ġmundo', '!', '[PAD]', '[PAD]', '[PAD]', '[PAD]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tokenizer(\"hola mundo!\", max_length=8, truncation=True, padding='max_length').tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claramente vemos los 4 tokens como cadenas independientes.\n",
    "\n",
    "### Definiendo el dataset de pytorch\n",
    "Ahora podemos proceder a definir el dataset. Esto debería ser muy sencillo dado que nuestro dataset es pequeño y ya tenemos el tokenizador listo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpanishNewsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer, dataset, seq_length: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.pad_token = '[PAD]'\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        # Definimos estos dos mapas para facilitarnos la tarea\n",
    "        # de traducir de nombres de categoría a ids de categoría.\n",
    "        self.id_2_class_map = dict(enumerate(np.unique(dataset[:]['category'])))\n",
    "        self.class_2_id_map = {v: k for k, v in self.id_2_class_map.items()}\n",
    "        self.num_classes = len(self.id_2_class_map)\n",
    "\n",
    "    def __getitem__(self, index) -> Dict[str, torch.Tensor]:\n",
    "        text, y = self.dataset[index]['text'], self.dataset[index]['category']\n",
    "        y = self.class_2_id_map[y]\n",
    "        data = {k: torch.tensor(v) for k, v in self.tokenizer(text, max_length=self.seq_length, truncation=True, padding='max_length').items()}\n",
    "        data['y'] = torch.tensor(y)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WikiANN_NER_Dataset(Dataset):\n",
    "    def __init__(self, tokenizer, dataset, label_pad_id=-100, seq_length: int = 128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.label_pad_id = label_pad_id\n",
    "\n",
    "        # Mapeo de etiquetas (ej: O, B-PER, I-PER...)\n",
    "        self.id_2_class = {\n",
    "            0: \"O\",\n",
    "            1: \"B-PER\",\n",
    "            2: \"I-PER\",\n",
    "            3: \"B-ORG\",\n",
    "            4: \"I-ORG\",\n",
    "            5: \"B-LOC\",\n",
    "            6: \"I-LOC\"\n",
    "        }\n",
    "        self.class_2_id = {v: k for k, v in self.id_2_class.items()}\n",
    "        self.num_classes = len(self.id_2_class)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = self.dataset[index]\n",
    "        tokens = example['tokens']\n",
    "        labels = example['ner_tags']\n",
    "\n",
    "        # Tokenizar preservando la alineación\n",
    "        tokenized = self.tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,  # importante para listas de tokens\n",
    "            max_length=self.seq_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Alinear etiquetas con subtokens\n",
    "        word_ids = tokenized.word_ids(batch_index=0)\n",
    "        aligned_labels = []\n",
    "        previous_word_idx = None\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                aligned_labels.append(self.label_pad_id)  # para tokens especiales y padding\n",
    "            elif word_idx != previous_word_idx:\n",
    "                aligned_labels.append(labels[word_idx])  # etiqueta original\n",
    "            else:\n",
    "                # Si quieres etiquetar subtokens con la misma etiqueta:\n",
    "                aligned_labels.append(labels[word_idx])\n",
    "                # O si quieres ignorarlos:\n",
    "                # aligned_labels.append(self.label_pad_id)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        # Convertir a tensores\n",
    "        item = {\n",
    "            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(aligned_labels)\n",
    "        }\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora instanciaremos el dataset entero. Para este experimento, definiremos un tamaño máximo de secuencia de 128 **tokens**  ya que nuestro dataset tiene registros de entre 3 y 64 tokens asi que deberia ser suficiente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "ner_dataset = WikiANN_NER_Dataset(ner_tokenizer, dataset, seq_length=max_len)\n",
    "assert len(ner_dataset) == len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y luego, procedemos a hacer el train-val-test split y crear los dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 4 \n",
    "train_dataset, val_dataset, test_dataset = random_split(ner_dataset, lengths=[0.8, 0.1, 0.1])\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de los Positional Embeddings\n",
    "\n",
    "Según el paper, los autores agregan una secuencia sinusoidal a los embeddings de los tokens con el fin de inyectar información referente a la posición de cada token en las frases. Esto obedece a la definición:\n",
    "\n",
    "$$\n",
    "PE(pos, 2i) = \\sin(pos/10000^{2i/d_{model}}) \\\\\n",
    "PE(pos, 2i + 1) = \\cos(pos/10000^{2i/d_{model}})\n",
    "$$\n",
    "\n",
    "Donde: \n",
    "- $pos$ es la posición del *token* en la secuencia.\n",
    "- $i$ es la dimensión $i$ en el embedding $d$.\n",
    "- $d_model$ es la dimensionalidad total del embedding.\n",
    "\n",
    "Lo que los autores propusieron fue que para las posiciones pares, se calculara el seno de la posición, relativa a la dimensionalidad del embedding y para las posiciones impares, se calculara el coseno. Según los autores, estos tenían la hipótesis de que estas funciones inyectarían la información posicional relativa de forma eficiente, en parte porque se pueden pre-calcular e inyectar directamente durante el entrenamiento, evitando asi emplear recursos en entrenar estructuras para aprenderlos.\n",
    "\n",
    "Esto último es particularmente importante ya que se evita tanto hacer uso de recursos innecesarios como acelerar el proceso de entrenamiento al no tener que computar gradientes para esta parte. Sin embargo, los autores también mencionaron que es ciertamente posible aprender estos positional embeddings como parte del entrenamiento y que según sus resultados, no había mucha diferencia entre ambos enfoques, razón por la cual, se prefiere el positional encoding sinusoidal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from enum import Enum\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class PosEncodingType(Enum):\n",
    "    SINUSOID = 1\n",
    "    LEARNABLE = 2\n",
    "\n",
    "\n",
    "class SinusoidPE(nn.Module):\n",
    "\n",
    "    def __init__(self, max_len: int, d_model: int):\n",
    "        super(SinusoidPE, self).__init__()\n",
    "        \n",
    "        # Definimos un vector columna con las posiciones de la secuencia de entrada (pos)\n",
    "        pos = torch.arange(max_len).unsqueeze(1)\n",
    "        # Definimos un vector de fila con las dimensiones del embedding (i)\n",
    "        i = torch.arange(d_model).unsqueeze(0)\n",
    "\n",
    "        # Calculamos el denominador segun la formula\n",
    "        div_term = 1 / torch.pow(10000, (2 * (i // 2)) / torch.tensor(d_model, dtype=torch.float32))\n",
    "        # Aplicamos el denominador a las posiciones\n",
    "        angle_rads = pos * div_term\n",
    "\n",
    "        # Inicializamos la matriz de positional encodings\n",
    "        pos_encoding = torch.zeros(max_len, d_model)\n",
    "        # Calculamos los embeddings para los numeros pares con seno: PE(pos, 2i)\n",
    "        pos_encoding[:, 0::2] = torch.sin(angle_rads[:, 0::2])\n",
    "        # Calculamos los embdeddings para los numeros inpares con coseno: PE(pos, 2i+1)\n",
    "        pos_encoding[:, 1::2] = torch.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        # Registramos la variable como atributo de clase\n",
    "        self.register_buffer(\"pos_encoding\", pos_encoding.unsqueeze(0), persistent=False)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.pos_encoding[:, :x.size(1), :]\n",
    "    \n",
    "\n",
    "class LearnablePE(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, d_model: int, max_len: int = float('-inf')):\n",
    "        super(LearnablePE, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        positions = torch.arange(0, max(x.size(-1), self.max_len))\n",
    "        pos_emb = self.embedding(positions)\n",
    "        return x + pos_emb\n",
    "\n",
    "\n",
    "\n",
    "class TokenAndPosEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, max_len: int, embed_dim: int, vocab_size: int, pos_encoding_type: PosEncodingType = PosEncodingType.SINUSOID):\n",
    "        super(TokenAndPosEmbedding, self).__init__()\n",
    "        self.token_emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "        if pos_encoding_type == PosEncodingType.SINUSOID:\n",
    "            self.pos_emb = SinusoidPE(max_len, embed_dim)\n",
    "        else:\n",
    "            self.pos_emb = LearnablePE(vocab_size, embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_emb = self.token_emb(x)\n",
    "        return self.pos_emb(token_emb)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procedemos a instanciar el modulo que va a convertir los tokens en embeddings con positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 128 \n",
    "tpe = TokenAndPosEmbedding(max_len, emb_dim, ner_tokenizer.vocab_size)\n",
    "pos_encoding = tpe.pos_emb.pos_encoding.squeeze(0).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A manera exploratoria, podemos observar gráficamente en que consisten estos vectores. En el siguiente gráfico podemos observar como los valores tienden a oscilar para diferentes posiciones en la dimensionalidad del embedding. Los valores individuales no tienen una interpretación directa, pero lo que vale la pena resaltar es que se observa una \"transición\" a medida que nos desplazamos por las dimensiones del embedding y sus respectivas posiciones, no es solo ruido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.pcolormesh(pos_encoding, cmap='viridis')\n",
    "plt.xlabel('Embedding Dimensions')\n",
    "plt.xlim((0, emb_dim))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, si pasamos nuestra frase simple por el tokenizador, deberíamos obtener una matriz con la forma: $(longitud, d_{model})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 128])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hola mundo!\"\n",
    "tokens = ner_tokenizer(text, max_length=max_len, truncation=True, padding='max_length')\n",
    "x = torch.tensor(tokens['input_ids']).unsqueeze(0)\n",
    "mask = torch.tensor(tokens['input_ids']).unsqueeze(0)\n",
    "embedding = tpe(x)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "![](../assets/mh-attention.png)\n",
    "\n",
    "Ahora procedemos a definir al núcleo del modelo. Recodemos que la atención se define por:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_K}})V\n",
    "$$\n",
    "\n",
    "Que es la definición de \"Scaled Dot-Product Attention\". Y Multi-Head Attention es la concatenación de varias cabezas ejecutando el mismo scaled dot-product sobre partes del input. Entonces tenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size: int, num_heads: int = 8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        assert embed_size & num_heads == 0, 'El tamaño del embedding debería ser divisible por el numero de cabezas'\n",
    "        self.projection_dim = embed_size // num_heads\n",
    "        self.query = nn.Linear(emb_dim, emb_dim)\n",
    "        self.key = nn.Linear(emb_dim, emb_dim)\n",
    "        self.value = nn.Linear(emb_dim, emb_dim)\n",
    "        self.comibe_heads = nn.Linear(emb_dim, emb_dim)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _scaled_dot_product(q, k, v, mask=None):\n",
    "        \"\"\"scaled dot product.\n",
    "\n",
    "        Esta función define el bloque mencionado.\n",
    "        Aquí se hace la multiplicación de matrices\n",
    "        entre los Q, K y V para luego calcular el \n",
    "        score de atención.\n",
    "\n",
    "        Nótese además que aquí aplicamos una máscara\n",
    "        de atención. Esto se debe a que como estamos\n",
    "        rellenando las cadenas cortas con un token que\n",
    "        en si mismo no trae ningún significado, no queremos\n",
    "        que la red desperdicie recursos operando sobre este\n",
    "        token, entonces usamos la máscara para poner los valores\n",
    "        de atención en numeros muy pequeños para que al\n",
    "        calcular el score, estos no sobresalgan sobre los demás.\n",
    "        \"\"\"\n",
    "        # d_k para el escalamiento\n",
    "        d_k = q.size()[-1]\n",
    "\n",
    "        # multiplicacion Q \\cdot K^T \n",
    "        attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "        # escalamiento\n",
    "        attn_logits = attn_logits / math.sqrt(d_k)\n",
    "        \n",
    "        # Se aplica la máscara\n",
    "        if mask is not None:\n",
    "            attn_logits = attn_logits.masked_fill(mask.reshape(mask.shape[0], 1, 1, -1) == 0, -9e-15)\n",
    "\n",
    "        # Se calcula el score de atención.\n",
    "        attention = torch.softmax(attn_logits, dim=-1)\n",
    "        # Se obtienen los valores tras el score de atención.\n",
    "        values = torch.matmul(attention, v)\n",
    "        return values, attention\n",
    "    \n",
    "\n",
    "    def _separate_heads(self, x, batch_size):\n",
    "        # Llega: (batch, seq_len, emb_dim)\n",
    "        x =  x.reshape(batch_size, -1, self.num_heads, self.projection_dim)  # (batch, seq_len, num_heads, emb_dim / num_heads)\n",
    "        return x.permute(0, 2, 1, 3)  # (batch, num_heads, seq_len, emb_dim / num_heads)\n",
    "    \n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        \"\"\"forward\n",
    "\n",
    "        Este es todo el forward pass del multi-head attention.\n",
    "        Aquí se coordina el resto de las operaciones, como\n",
    "        la concatenación de las múltiples cabezas como \n",
    "        el paso por la capa densa previo a entregar el\n",
    "        resultado.\n",
    "        \"\"\"\n",
    "        # x: (batch, seq_len, emb_dim)\n",
    "        batch_size, seq_len, emb_dim = x.size()\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        q = self._separate_heads(q, batch_size)\n",
    "        k = self._separate_heads(k, batch_size)\n",
    "        v = self._separate_heads(v, batch_size)\n",
    "\n",
    "        weights, attention = self._scaled_dot_product(q, k, v, mask)\n",
    "        weights = weights.permute(0, 2, 1, 3) # (batch, seq_len, num_heads, emb_dim / num_heads)\n",
    "        weights = weights.reshape(batch_size, seq_len, emb_dim)\n",
    "        output = self.comibe_heads(weights)\n",
    "        \n",
    "        if return_attention:\n",
    "            return output, attention\n",
    "        else:\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos hacer una prueba rápida de que las operaciones funcionan a nivel de matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiHeadAttention(emb_dim)\n",
    "mha(embedding, mask).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición del bloque transformers\n",
    "\n",
    "![](../assets/transformers-achitecture.png)\n",
    "\n",
    "Finalmente, definimos el bloque de transformers. Recordemos que como esta es una tarea de clasificación, solamente necesitamos el encoder, por lo que esto es solamente la primera parte del diseño de arquitecura de red.\n",
    "\n",
    "En esta capa, simplemente ponemos una capa densa adicional junto con las normalizaciones a nivel de capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim: int, num_heads: int = 8, ff_dim: int = 512, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhatt = MultiHeadAttention(emb_dim, num_heads)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(emb_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, emb_dim)\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Multi-head attention with residual\n",
    "        attn_output = self.mhatt(x, mask)\n",
    "        x = x + self.dropout1(attn_output)\n",
    "        x = self.layer_norm1(x)\n",
    "        \n",
    "        # Feed-forward with residual\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = x + self.dropout2(ffn_output)\n",
    "        x = self.layer_norm2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, probamos rapidamente para asegurarnos que las capas operan correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 128])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb = TransformerBlock(emb_dim)\n",
    "tb(embedding, mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''num_heads = 8\n",
    "vocab_size = spanish_news_tokenizer.vocab_size\n",
    "\n",
    "token_embeddings = TokenAndPosEmbedding(max_len, emb_dim, vocab_size)\n",
    "transformer = TransformerBlock(emb_dim, num_heads)\n",
    "ff = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(max_len * emb_dim, spanish_news_dataset.num_classes)\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_heads=8\n",
    "labels = dataset.features[\"ner_tags\"].feature.names\n",
    "vocab_size = ner_tokenizer.vocab_size\n",
    "\n",
    "num_labels = len(labels)  # Example: WikiANN has ~4 main classes (O, PER, ORG, LOC)\n",
    "\n",
    "token_embeddings = TokenAndPosEmbedding(max_len, emb_dim, vocab_size)\n",
    "transformer = TransformerBlock(emb_dim, num_heads)\n",
    "\n",
    "# Instead of flattening, project each token embedding to num_labels\n",
    "classifier = nn.Linear(emb_dim, num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([4, 128, 128])\n",
      "Transformer output shape: torch.Size([4, 128, 128])\n",
      "Logits shape: torch.Size([4, 128, 7])\n",
      "Labels shape: torch.Size([4, 128])\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_loader)\n",
    "batch = next(it)\n",
    "\n",
    "x, mask, y = batch['input_ids'], batch['attention_mask'], batch['labels']  # rename 'y' -> 'labels' if needed\n",
    "\n",
    "# Embeddings\n",
    "embeddings = token_embeddings(x)\n",
    "print(\"Embeddings shape:\", embeddings.shape)  # (batch_size, max_len, emb_dim)\n",
    "\n",
    "# Transformer output\n",
    "attention = transformer(embeddings, mask)\n",
    "print(\"Transformer output shape:\", attention.shape)  # (batch_size, max_len, emb_dim)\n",
    "\n",
    "# Classifier output\n",
    "logits = classifier(attention)\n",
    "print(\"Logits shape:\", logits.shape)  # (batch_size, max_len, num_labels)\n",
    "\n",
    "# Check labels shape\n",
    "print(\"Labels shape:\", y.shape)  # (batch_size, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 7])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = classifier(attention)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición del clasificador\n",
    "\n",
    "Finalmente, definimos el modelo en si. Este modelo constará de 3 capas:\n",
    "\n",
    "- La tokenización, tal como la definimos anteriormente.\n",
    "- El transformer, que acabamos de decinir.\n",
    "- Una capa densa adicional que servirá como clasificador de aquello que nos entregue la capa del transformer.\n",
    "\n",
    "Como este es un LightningModule, aquí definiremos el resto de funciones utilitarias para el entrenamiento de la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import LightningModule\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "class WikiANNTransformerNER(LightningModule):\n",
    "\n",
    "    def __init__(self, max_len: int, vocab_size: int, num_labels: int, emb_dim: int, num_heads: int = 8, pad_idx: int = -100):\n",
    "        super(WikiANNTransformerNER, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.token_embeddings = TokenAndPosEmbedding(max_len, emb_dim, vocab_size)\n",
    "        self.transformer = TransformerBlock(emb_dim, num_heads)\n",
    "        self.classifier = nn.Linear(emb_dim, num_labels)\n",
    "\n",
    "        # Metrics (compute accuracy ignoring padding)\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_labels, ignore_index=pad_idx)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_labels, ignore_index=pad_idx)\n",
    "        self.test_acc = Accuracy(task=\"multiclass\", num_classes=num_labels, ignore_index=pad_idx)\n",
    "\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        embeddings = self.token_embeddings(x)  # (B, L, D)\n",
    "        attention = self.transformer(embeddings, mask)  # (B, L, D)\n",
    "        logits = self.classifier(attention)  # (B, L, num_labels)\n",
    "        return logits\n",
    "\n",
    "    def shared_step(self, batch, stage: str):\n",
    "        x, mask, y = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "        logits = self(x, mask)  # (B, L, num_labels)\n",
    "\n",
    "        # Reshape for loss (flatten sequences)\n",
    "        loss = F.cross_entropy(logits.view(-1, self.hparams.num_labels),\n",
    "                               y.view(-1),\n",
    "                               ignore_index=self.pad_idx)\n",
    "\n",
    "        # Compute accuracy ignoring padding\n",
    "        self.log(f\"{stage}-loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss, logits, y\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, logits, y = self.shared_step(batch, \"train\")\n",
    "        self.train_acc(logits.view(-1, self.hparams.num_labels), y.view(-1))\n",
    "        self.log(\"train-acc\", self.train_acc, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, logits, y = self.shared_step(batch, \"val\")\n",
    "        self.val_acc(logits.view(-1, self.hparams.num_labels), y.view(-1))\n",
    "        self.log(\"val-acc\", self.val_acc, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, logits, y = self.shared_step(batch, \"test\")\n",
    "        self.test_acc(logits.view(-1, self.hparams.num_labels), y.view(-1))\n",
    "        self.log(\"test-acc\", self.test_acc, prog_bar=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=2e-5, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                 | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | token_embeddings | TokenAndPosEmbedding | 2.6 M  | train\n",
      "1 | transformer      | TransformerBlock     | 198 K  | train\n",
      "2 | classifier       | Linear               | 903    | train\n",
      "3 | train_acc        | MulticlassAccuracy   | 0      | train\n",
      "4 | val_acc          | MulticlassAccuracy   | 0      | train\n",
      "5 | test_acc         | MulticlassAccuracy   | 0      | train\n",
      "------------------------------------------------------------------\n",
      "2.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 M     Total params\n",
      "11.037    Total estimated model params size (MB)\n",
      "22        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4000/4000 [00:28<00:00, 141.27it/s, v_num=1, train-acc_step=1.000, val-loss=0.775, val-acc=0.720, train-loss=0.697, train-acc_epoch=0.751]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4000/4000 [00:28<00:00, 140.67it/s, v_num=1, train-acc_step=1.000, val-loss=0.775, val-acc=0.720, train-loss=0.697, train-acc_epoch=0.751]\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(dataset.features[\"ner_tags\"].feature.names)\n",
    "pad_idx = -100  # HuggingFace datasets usually use this for ignored tokens\n",
    "\n",
    "model = WikiANNTransformerNER(\n",
    "    max_len=max_len,\n",
    "    vocab_size=ner_tokenizer.vocab_size,\n",
    "    num_labels=num_labels,\n",
    "    emb_dim=emb_dim,\n",
    "    num_heads=8,\n",
    "    pad_idx=pad_idx\n",
    ")\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "tb_logger = TensorBoardLogger(\"tb_logs\", name=\"WikiANN-TransformerNER\")\n",
    "callbacks = [EarlyStopping(monitor=\"val-loss\", patience=3, mode=\"min\")]\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=20,\n",
    "    devices=1,\n",
    "    logger=tb_logger,\n",
    "    callbacks=callbacks,\n",
    "    precision=\"16-mixed\"\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observemos el proceso de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir tb_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y como es de esperarse, realizaremos la validación contra el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 500/500 [00:01<00:00, 317.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test-acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7082191109657288     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test-loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8160772919654846     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test-acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7082191109657288    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test-loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8160772919654846    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test-loss': 0.8160772919654846, 'test-acc': 0.7082191109657288}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haciendo predicciones\n",
    "\n",
    "Finalmente, vamos a hacer uso del modelo y ver que tan bueno es para la clasificación de noticias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m predictions = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m predictions = torch.cat(predictions, dim=\u001b[32m0\u001b[39m)\n\u001b[32m      3\u001b[39m predictions = torch.argmax(predictions, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:886\u001b[39m, in \u001b[36mTrainer.predict\u001b[39m\u001b[34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[39m\n\u001b[32m    884\u001b[39m \u001b[38;5;28mself\u001b[39m.state.status = TrainerStatus.RUNNING\n\u001b[32m    885\u001b[39m \u001b[38;5;28mself\u001b[39m.predicting = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m886\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:49\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     48\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     52\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:927\u001b[39m, in \u001b[36mTrainer._predict_impl\u001b[39m\u001b[34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[39m\n\u001b[32m    923\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    924\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    925\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn, ckpt_path, model_provided=model_provided, model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    926\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    930\u001b[39m \u001b[38;5;28mself\u001b[39m.predicting = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1011\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1008\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1011\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1016\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1050\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1048\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._evaluation_loop.run()\n\u001b[32m   1049\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predicting:\n\u001b[32m-> \u001b[39m\u001b[32m1050\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1052\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/loops/prediction_loop.py:125\u001b[39m, in \u001b[36m_PredictionLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_progress.is_last_batch = data_fetcher.done\n\u001b[32m    124\u001b[39m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    127\u001b[39m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/loops/prediction_loop.py:255\u001b[39m, in \u001b[36m_PredictionLoop._predict_step\u001b[39m\u001b[34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# configure step_kwargs\u001b[39;00m\n\u001b[32m    250\u001b[39m step_args = (\n\u001b[32m    251\u001b[39m     \u001b[38;5;28mself\u001b[39m._build_step_args_from_hook_kwargs(hook_kwargs, \u001b[33m\"\u001b[39m\u001b[33mpredict_step\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[32m    253\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[32m    254\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m predictions = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpredict_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m predictions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    257\u001b[39m     \u001b[38;5;28mself\u001b[39m._warning_cache.warn(\u001b[33m\"\u001b[39m\u001b[33mpredict returned None if it was on purpose, ignore this warning...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:329\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    332\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:438\u001b[39m, in \u001b[36mStrategy.predict_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    437\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mpredict_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/core/module.py:966\u001b[39m, in \u001b[36mLightningModule.predict_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    964\u001b[39m \u001b[38;5;66;03m# For backwards compatibility\u001b[39;00m\n\u001b[32m    965\u001b[39m batch = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m, args[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mWikiANNTransformerNER.forward\u001b[39m\u001b[34m(self, x, mask)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, L, D)\u001b[39;00m\n\u001b[32m     26\u001b[39m     attention = \u001b[38;5;28mself\u001b[39m.transformer(embeddings, mask)  \u001b[38;5;66;03m# (B, L, D)\u001b[39;00m\n\u001b[32m     27\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.classifier(attention)  \u001b[38;5;66;03m# (B, L, num_labels)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mTokenAndPosEmbedding.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     token_emb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pos_emb(token_emb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/sparse.py:163\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/functional.py:2237\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2231\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2232\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2233\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2234\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2235\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2236\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: embedding(): argument 'indices' (position 2) must be Tensor, not dict"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(model, test_loader)\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "predictions = torch.argmax(predictions, dim=-1)\n",
    "predictions = [ner_dataset.id_2_class_map[pred] for pred in predictions.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   0%|          | 0/500 [06:57<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 1. Hacer predicciones\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m predictions = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# lista de batches\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 2. Concatenar todos los batches\u001b[39;00m\n\u001b[32m      5\u001b[39m predictions = torch.cat(predictions, dim=\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# shape (num_examples, seq_len, num_labels)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:886\u001b[39m, in \u001b[36mTrainer.predict\u001b[39m\u001b[34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[39m\n\u001b[32m    884\u001b[39m \u001b[38;5;28mself\u001b[39m.state.status = TrainerStatus.RUNNING\n\u001b[32m    885\u001b[39m \u001b[38;5;28mself\u001b[39m.predicting = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m886\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:49\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     48\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     52\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:927\u001b[39m, in \u001b[36mTrainer._predict_impl\u001b[39m\u001b[34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[39m\n\u001b[32m    923\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    924\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    925\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn, ckpt_path, model_provided=model_provided, model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    926\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    930\u001b[39m \u001b[38;5;28mself\u001b[39m.predicting = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1011\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1008\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1011\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1016\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1050\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1048\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._evaluation_loop.run()\n\u001b[32m   1049\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predicting:\n\u001b[32m-> \u001b[39m\u001b[32m1050\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1052\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/loops/prediction_loop.py:125\u001b[39m, in \u001b[36m_PredictionLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_progress.is_last_batch = data_fetcher.done\n\u001b[32m    124\u001b[39m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    127\u001b[39m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/loops/prediction_loop.py:255\u001b[39m, in \u001b[36m_PredictionLoop._predict_step\u001b[39m\u001b[34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# configure step_kwargs\u001b[39;00m\n\u001b[32m    250\u001b[39m step_args = (\n\u001b[32m    251\u001b[39m     \u001b[38;5;28mself\u001b[39m._build_step_args_from_hook_kwargs(hook_kwargs, \u001b[33m\"\u001b[39m\u001b[33mpredict_step\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[32m    253\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[32m    254\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m predictions = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpredict_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m predictions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    257\u001b[39m     \u001b[38;5;28mself\u001b[39m._warning_cache.warn(\u001b[33m\"\u001b[39m\u001b[33mpredict returned None if it was on purpose, ignore this warning...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:329\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    332\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:438\u001b[39m, in \u001b[36mStrategy.predict_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    437\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mpredict_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/pytorch_lightning/core/module.py:966\u001b[39m, in \u001b[36mLightningModule.predict_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    964\u001b[39m \u001b[38;5;66;03m# For backwards compatibility\u001b[39;00m\n\u001b[32m    965\u001b[39m batch = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m, args[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mWikiANNTransformerNER.forward\u001b[39m\u001b[34m(self, x, mask)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, L, D)\u001b[39;00m\n\u001b[32m     26\u001b[39m     attention = \u001b[38;5;28mself\u001b[39m.transformer(embeddings, mask)  \u001b[38;5;66;03m# (B, L, D)\u001b[39;00m\n\u001b[32m     27\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.classifier(attention)  \u001b[38;5;66;03m# (B, L, num_labels)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mTokenAndPosEmbedding.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     token_emb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pos_emb(token_emb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/sparse.py:163\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/functional.py:2237\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2231\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2232\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2233\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2234\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2235\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2236\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: embedding(): argument 'indices' (position 2) must be Tensor, not dict"
     ]
    }
   ],
   "source": [
    "# 1. Hacer predicciones\n",
    "predictions = trainer.predict(model, test_loader)  # lista de batches\n",
    "\n",
    "# 2. Concatenar todos los batches\n",
    "predictions = torch.cat(predictions, dim=0)  # shape (num_examples, seq_len, num_labels)\n",
    "\n",
    "# 3. Obtener la clase con mayor score por token\n",
    "predicted_ids = torch.argmax(predictions, dim=-1)  # shape (num_examples, seq_len)\n",
    "\n",
    "# 4. Mapear a etiquetas NER reales, ignorando padding tokens\n",
    "predicted_labels = []\n",
    "for i, seq in enumerate(predicted_ids):\n",
    "    seq_labels = []\n",
    "    for j, token_id in enumerate(seq):\n",
    "        # asumiendo que el padding de labels es -100\n",
    "        if ner_dataset.labels[i][j] != -100:  \n",
    "            seq_labels.append(ner_dataset.id_2_class_map[token_id.item()])\n",
    "    predicted_labels.append(seq_labels)\n",
    "\n",
    "# predicted_labels es ahora una lista de listas de etiquetas por ejemplo\n",
    "print(predicted_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added predict_step method to WikiANNTransformerNER class\n",
      "Running predictions...\n",
      "Predicting DataLoader 0: 100%|██████████| 500/500 [00:01<00:00, 453.41it/s]\n",
      "Got predictions for 500 batches\n",
      "Processed 16413 predictions\n",
      "Processed 16413 true labels\n",
      "Overall Accuracy: 0.7082\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.64      0.55      0.59      1242\n",
      "       B-ORG       0.62      0.43      0.51      1067\n",
      "       B-PER       0.55      0.51      0.53      1068\n",
      "       I-LOC       0.61      0.48      0.54      1564\n",
      "       I-ORG       0.68      0.65      0.66      3077\n",
      "       I-PER       0.52      0.61      0.56      2178\n",
      "           O       0.85      0.95      0.89      6217\n",
      "\n",
      "    accuracy                           0.71     16413\n",
      "   macro avg       0.64      0.59      0.61     16413\n",
      "weighted avg       0.70      0.71      0.70     16413\n",
      "\n",
      "\n",
      "Entity-only Accuracy: 0.5631\n",
      "\n",
      "Entity Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.67      0.55      0.60      1242\n",
      "       B-ORG       0.64      0.43      0.51      1067\n",
      "       B-PER       0.58      0.51      0.54      1068\n",
      "       I-LOC       0.62      0.48      0.54      1564\n",
      "       I-ORG       0.70      0.65      0.67      3077\n",
      "       I-PER       0.54      0.61      0.57      2178\n",
      "           O       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.56     10196\n",
      "   macro avg       0.54      0.46      0.49     10196\n",
      "weighted avg       0.63      0.56      0.59     10196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== CELL 1: Fix the predict_step method =====\n",
    "# Add this method to your WikiANNTransformerNER class\n",
    "\n",
    "def predict_step(self, batch, batch_idx):\n",
    "    \"\"\"Prediction step for inference - fixed to handle batch properly\"\"\"\n",
    "    # Extract tensors from batch dictionary\n",
    "    x = batch['input_ids']\n",
    "    mask = batch['attention_mask']\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = self(x, mask)  # Now calling forward with proper arguments\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'attention_mask': mask,\n",
    "        'labels': batch.get('labels', None)\n",
    "    }\n",
    "\n",
    "# Add the method to your existing class\n",
    "WikiANNTransformerNER.predict_step = predict_step\n",
    "\n",
    "print(\"Added predict_step method to WikiANNTransformerNER class\")\n",
    "\n",
    "# ===== CELL 2: Run predictions (this should work now) =====\n",
    "print(\"Running predictions...\")\n",
    "predictions_list = trainer.predict(model, test_loader)\n",
    "print(f\"Got predictions for {len(predictions_list)} batches\")\n",
    "\n",
    "# ===== CELL 3: Process predictions properly =====\n",
    "def process_predictions(predictions_list, dataset, id_2_class_map):\n",
    "    \"\"\"Process the predictions from trainer.predict()\"\"\"\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    \n",
    "    for batch_results in predictions_list:\n",
    "        pred_ids = batch_results['predictions']  # (batch_size, seq_len)\n",
    "        attention_masks = batch_results['attention_mask']  # (batch_size, seq_len)\n",
    "        true_labels = batch_results.get('labels', None)\n",
    "        \n",
    "        # Process each sequence in the batch\n",
    "        for i in range(pred_ids.size(0)):\n",
    "            seq_preds = pred_ids[i]\n",
    "            seq_mask = attention_masks[i]\n",
    "            \n",
    "            # Get only valid (non-padded) predictions\n",
    "            valid_preds = seq_preds[seq_mask == 1]\n",
    "            \n",
    "            # Convert to label names\n",
    "            pred_labels = [id_2_class_map.get(pred.item(), 'O') for pred in valid_preds]\n",
    "            all_predictions.extend(pred_labels)\n",
    "            \n",
    "            # Process true labels if available\n",
    "            if true_labels is not None:\n",
    "                seq_true = true_labels[i]\n",
    "                valid_true = seq_true[seq_mask == 1]\n",
    "                # Remove padding labels (-100)\n",
    "                valid_true = valid_true[valid_true != -100]\n",
    "                true_label_names = [id_2_class_map.get(label.item(), 'O') for label in valid_true]\n",
    "                all_true_labels.extend(true_label_names)\n",
    "    \n",
    "    return all_predictions, all_true_labels if all_true_labels else None\n",
    "\n",
    "# Use the id_2_class mapping from your dataset\n",
    "id_2_class_map = train_dataset.dataset.id_2_class\n",
    "predictions, true_labels = process_predictions(predictions_list, test_dataset, id_2_class_map)\n",
    "\n",
    "print(f\"Processed {len(predictions)} predictions\")\n",
    "if true_labels:\n",
    "    print(f\"Processed {len(true_labels)} true labels\")\n",
    "\n",
    "# ===== CELL 4: Calculate metrics =====\n",
    "if true_labels:\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, predictions))\n",
    "    \n",
    "    # Entity-level metrics (excluding 'O' tags)\n",
    "    entity_true = [label for label in true_labels if label != 'O']\n",
    "    entity_pred = [pred for pred, true in zip(predictions, true_labels) if true != 'O']\n",
    "    \n",
    "    if entity_true:\n",
    "        entity_accuracy = accuracy_score(entity_true, entity_pred)\n",
    "        print(f\"\\nEntity-only Accuracy: {entity_accuracy:.4f}\")\n",
    "        print(\"\\nEntity Classification Report:\")\n",
    "        print(classification_report(entity_true, entity_pred))\n",
    "\n",
    "# ===== CELL 5: Show sample predictions with original tokens =====\n",
    "def show_detailed_predictions(model, test_dataset, test_loader, trainer, num_samples=3):\n",
    "    \"\"\"Show predictions aligned with original tokens\"\"\"\n",
    "    \n",
    "    predictions_list = trainer.predict(model, test_loader)\n",
    "    \n",
    "    sample_count = 0\n",
    "    batch_idx = 0\n",
    "    \n",
    "    for batch_results in predictions_list:\n",
    "        if sample_count >= num_samples:\n",
    "            break\n",
    "            \n",
    "        pred_ids = batch_results['predictions']\n",
    "        attention_masks = batch_results['attention_mask']\n",
    "        \n",
    "        for i in range(pred_ids.size(0)):\n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "                \n",
    "            # Calculate the actual index in the dataset\n",
    "            actual_idx = batch_idx * test_loader.batch_size + i\n",
    "            if actual_idx >= len(test_dataset):\n",
    "                break\n",
    "                \n",
    "            # Get original example\n",
    "            original_example = test_dataset.dataset[test_dataset.indices[actual_idx]]\n",
    "            original_tokens = original_example['tokens']\n",
    "            original_labels = [test_dataset.dataset.id_2_class[label] for label in original_example['ner_tags']]\n",
    "            \n",
    "            # Get predictions for this sequence\n",
    "            seq_predictions = pred_ids[i]\n",
    "            seq_mask = attention_masks[i]\n",
    "            \n",
    "            # Filter predictions by attention mask\n",
    "            valid_predictions = seq_predictions[seq_mask == 1]\n",
    "            pred_labels = [test_dataset.dataset.id_2_class.get(pred.item(), 'O') for pred in valid_predictions]\n",
    "            \n",
    "            # Align with original tokens (take first N predictions where N = len(original_tokens))\n",
    "            aligned_predictions = pred_labels[:len(original_tokens)]\n",
    "            \n",
    "            print(f\"\\n=== Sample {sample_count + 1} ===\")\n",
    "            print(\"Token        | True       | Predicted  | Match\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for j, token in enumerate(original_tokens):\n",
    "                true_label = original_labels[j] if j < len(original_labels) else 'O'\n",
    "                pred_label = aligned_predictions[j] if j < len(aligned_predictions) else 'O'\n",
    "                match_symbol = \"✓\" if true_label == pred_label else \"✗\"\n",
    "                print(f\"{token:12} | {true_label:10} | {pred_label:10} | {match_symbol}\")\n",
    "            \n",
    "            sample_count += 1\n",
    "        \n",
    "        batch_idx += 1\n",
    "    \n",
    "    return sample_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping predictions by sample...\n",
      "Grouped into 2000 samples\n",
      "DataFrame created with 2000 samples\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_ids</th>\n",
       "      <th>token_strings</th>\n",
       "      <th>true_labels</th>\n",
       "      <th>predicted_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16138</th>\n",
       "      <td>[Escala, de, huracanes, de, Saffir-Simpson]</td>\n",
       "      <td>[794, 703, 260, 8954, 2223, 289, 260, 694, 867...</td>\n",
       "      <td>[ĠEsc, ala, Ġde, Ġhura, can, es, Ġde, ĠSa, ff,...</td>\n",
       "      <td>[B-ORG, I-ORG, I-ORG, I-ORG, I-ORG]</td>\n",
       "      <td>[B-ORG, I-ORG, I-ORG, I-ORG, I-ORG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19249</th>\n",
       "      <td>[*El, primer, cuartel, simboliza, las, Islas, ...</td>\n",
       "      <td>[438, 3227, 2198, 990, 17310, 6294, 434, 1445,...</td>\n",
       "      <td>[Ġ*, El, Ġprimer, Ġcu, artel, Ġsim, bol, iza, ...</td>\n",
       "      <td>[O, O, O, O, O, B-LOC, I-LOC, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5649</th>\n",
       "      <td>[REDIRECCIÓN, Tubérculo, menor, del, húmero]</td>\n",
       "      <td>[276, 10777, 6464, 280, 360, 8957]</td>\n",
       "      <td>[ĠREDIRECCIÃĵN, ĠTubÃ©rculo, Ġmen, or, Ġdel, Ġ...</td>\n",
       "      <td>[O, B-LOC, I-LOC, I-LOC, I-LOC]</td>\n",
       "      <td>[O, B-LOC, B-LOC, I-LOC, I-LOC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11211</th>\n",
       "      <td>[', '', Coquimbo, Unido, '', ']</td>\n",
       "      <td>[257, 262, 4381, 1312, 262, 257]</td>\n",
       "      <td>[Ġ', Ġ'', ĠCoquimbo, ĠUnido, Ġ'', Ġ']</td>\n",
       "      <td>[O, O, B-ORG, I-ORG, O, O]</td>\n",
       "      <td>[O, O, I-PER, I-PER, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18077</th>\n",
       "      <td>[====, Circuitos, Continentales, UCI, ====]</td>\n",
       "      <td>[2051, 3184, 5132, 15376, 3048, 6286, 2051]</td>\n",
       "      <td>[Ġ====, ĠCircu, itos, ĠContin, entales, ĠUCI, ...</td>\n",
       "      <td>[O, B-ORG, I-ORG, I-ORG, O]</td>\n",
       "      <td>[O, B-PER, I-PER, I-PER, I-PER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12921</th>\n",
       "      <td>[junto, a, Ben, Chaplin, y, Idris, Elba]</td>\n",
       "      <td>[1009, 342, 839, 2623, 1979, 337, 6254, 865, 1...</td>\n",
       "      <td>[Ġjunto, Ġa, ĠBen, ĠChap, lin, Ġy, ĠId, ris, Ġ...</td>\n",
       "      <td>[O, O, B-PER, I-PER, O, B-PER, I-PER]</td>\n",
       "      <td>[O, O, B-PER, I-PER, I-ORG, I-PER, I-PER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9350</th>\n",
       "      <td>[REDIRECCIÓN, The, World, We, Live, In]</td>\n",
       "      <td>[276, 592, 1803, 2445, 2398, 430]</td>\n",
       "      <td>[ĠREDIRECCIÃĵN, ĠThe, ĠWorld, ĠWe, ĠLive, ĠIn]</td>\n",
       "      <td>[O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG]</td>\n",
       "      <td>[O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12190</th>\n",
       "      <td>[===, Universal, /, PolyGram, ===]</td>\n",
       "      <td>[761, 4386, 499, 885, 89, 39, 4992, 761]</td>\n",
       "      <td>[Ġ===, ĠUniversal, Ġ/, ĠPol, y, G, ram, Ġ===]</td>\n",
       "      <td>[O, B-ORG, O, B-ORG, O]</td>\n",
       "      <td>[O, I-PER, O, I-ORG, I-PER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12414</th>\n",
       "      <td>[Partido, Comunista, de, Alemania, (, Oposició...</td>\n",
       "      <td>[575, 2191, 260, 541, 286, 15022, 287]</td>\n",
       "      <td>[ĠPartido, ĠComunista, Ġde, ĠAlemania, Ġ(, ĠOp...</td>\n",
       "      <td>[B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG]</td>\n",
       "      <td>[B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10482</th>\n",
       "      <td>[Santa, María, del, Oro]</td>\n",
       "      <td>[723, 547, 360, 1770]</td>\n",
       "      <td>[ĠSanta, ĠMarÃŃa, Ġdel, ĠOro]</td>\n",
       "      <td>[B-LOC, I-LOC, I-LOC, I-LOC]</td>\n",
       "      <td>[B-LOC, I-PER, I-PER, I-PER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6427</th>\n",
       "      <td>[Comarca, de, El, Morrazo]</td>\n",
       "      <td>[2646, 260, 437, 18436]</td>\n",
       "      <td>[ĠComarca, Ġde, ĠEl, ĠMorrazo]</td>\n",
       "      <td>[B-LOC, I-LOC, I-LOC, I-LOC]</td>\n",
       "      <td>[B-LOC, I-LOC, B-LOC, I-LOC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5138</th>\n",
       "      <td>[REDIRECCIÓN, Sebastián, Cirac, Estopañán]</td>\n",
       "      <td>[276, 2686, 18119, 6313, 19617]</td>\n",
       "      <td>[ĠREDIRECCIÃĵN, ĠSebastiÃ¡n, ĠCirac, ĠEsto, pa...</td>\n",
       "      <td>[O, B-PER, I-PER, I-PER]</td>\n",
       "      <td>[O, B-LOC, B-PER, I-PER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14327</th>\n",
       "      <td>[**, Perales, de, Tajuña, .]</td>\n",
       "      <td>[453, 605, 608, 260, 19388, 288]</td>\n",
       "      <td>[Ġ**, ĠPer, ales, Ġde, ĠTajuÃ±a, Ġ.]</td>\n",
       "      <td>[O, B-LOC, I-LOC, I-LOC, O]</td>\n",
       "      <td>[O, B-LOC, O, I-LOC, B-LOC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>[', '', Rennae, Stubbs, Helena, Suková]</td>\n",
       "      <td>[257, 262, 10137, 9268, 6241, 9006]</td>\n",
       "      <td>[Ġ', Ġ'', ĠRennae, ĠStubbs, ĠHelena, ĠSukovÃ¡]</td>\n",
       "      <td>[O, O, B-PER, I-PER, B-PER, I-PER]</td>\n",
       "      <td>[O, O, B-PER, B-ORG, B-LOC, I-ORG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7594</th>\n",
       "      <td>[José, María, Zeledón, Brenes]</td>\n",
       "      <td>[482, 547, 9827, 17693]</td>\n",
       "      <td>[ĠJosÃ©, ĠMarÃŃa, ĠZeledÃ³n, ĠBrenes]</td>\n",
       "      <td>[B-PER, I-PER, I-PER, I-PER]</td>\n",
       "      <td>[B-PER, I-PER, I-PER, I-PER]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tokens  \\\n",
       "16138        [Escala, de, huracanes, de, Saffir-Simpson]   \n",
       "19249  [*El, primer, cuartel, simboliza, las, Islas, ...   \n",
       "5649        [REDIRECCIÓN, Tubérculo, menor, del, húmero]   \n",
       "11211                    [', '', Coquimbo, Unido, '', ']   \n",
       "18077        [====, Circuitos, Continentales, UCI, ====]   \n",
       "12921           [junto, a, Ben, Chaplin, y, Idris, Elba]   \n",
       "9350             [REDIRECCIÓN, The, World, We, Live, In]   \n",
       "12190                 [===, Universal, /, PolyGram, ===]   \n",
       "12414  [Partido, Comunista, de, Alemania, (, Oposició...   \n",
       "10482                           [Santa, María, del, Oro]   \n",
       "6427                          [Comarca, de, El, Morrazo]   \n",
       "5138          [REDIRECCIÓN, Sebastián, Cirac, Estopañán]   \n",
       "14327                       [**, Perales, de, Tajuña, .]   \n",
       "1906             [', '', Rennae, Stubbs, Helena, Suková]   \n",
       "7594                      [José, María, Zeledón, Brenes]   \n",
       "\n",
       "                                               token_ids  \\\n",
       "16138  [794, 703, 260, 8954, 2223, 289, 260, 694, 867...   \n",
       "19249  [438, 3227, 2198, 990, 17310, 6294, 434, 1445,...   \n",
       "5649                  [276, 10777, 6464, 280, 360, 8957]   \n",
       "11211                   [257, 262, 4381, 1312, 262, 257]   \n",
       "18077        [2051, 3184, 5132, 15376, 3048, 6286, 2051]   \n",
       "12921  [1009, 342, 839, 2623, 1979, 337, 6254, 865, 1...   \n",
       "9350                   [276, 592, 1803, 2445, 2398, 430]   \n",
       "12190           [761, 4386, 499, 885, 89, 39, 4992, 761]   \n",
       "12414             [575, 2191, 260, 541, 286, 15022, 287]   \n",
       "10482                              [723, 547, 360, 1770]   \n",
       "6427                             [2646, 260, 437, 18436]   \n",
       "5138                     [276, 2686, 18119, 6313, 19617]   \n",
       "14327                   [453, 605, 608, 260, 19388, 288]   \n",
       "1906                 [257, 262, 10137, 9268, 6241, 9006]   \n",
       "7594                             [482, 547, 9827, 17693]   \n",
       "\n",
       "                                           token_strings  \\\n",
       "16138  [ĠEsc, ala, Ġde, Ġhura, can, es, Ġde, ĠSa, ff,...   \n",
       "19249  [Ġ*, El, Ġprimer, Ġcu, artel, Ġsim, bol, iza, ...   \n",
       "5649   [ĠREDIRECCIÃĵN, ĠTubÃ©rculo, Ġmen, or, Ġdel, Ġ...   \n",
       "11211              [Ġ', Ġ'', ĠCoquimbo, ĠUnido, Ġ'', Ġ']   \n",
       "18077  [Ġ====, ĠCircu, itos, ĠContin, entales, ĠUCI, ...   \n",
       "12921  [Ġjunto, Ġa, ĠBen, ĠChap, lin, Ġy, ĠId, ris, Ġ...   \n",
       "9350      [ĠREDIRECCIÃĵN, ĠThe, ĠWorld, ĠWe, ĠLive, ĠIn]   \n",
       "12190      [Ġ===, ĠUniversal, Ġ/, ĠPol, y, G, ram, Ġ===]   \n",
       "12414  [ĠPartido, ĠComunista, Ġde, ĠAlemania, Ġ(, ĠOp...   \n",
       "10482                      [ĠSanta, ĠMarÃŃa, Ġdel, ĠOro]   \n",
       "6427                      [ĠComarca, Ġde, ĠEl, ĠMorrazo]   \n",
       "5138   [ĠREDIRECCIÃĵN, ĠSebastiÃ¡n, ĠCirac, ĠEsto, pa...   \n",
       "14327               [Ġ**, ĠPer, ales, Ġde, ĠTajuÃ±a, Ġ.]   \n",
       "1906      [Ġ', Ġ'', ĠRennae, ĠStubbs, ĠHelena, ĠSukovÃ¡]   \n",
       "7594               [ĠJosÃ©, ĠMarÃŃa, ĠZeledÃ³n, ĠBrenes]   \n",
       "\n",
       "                                             true_labels  \\\n",
       "16138                [B-ORG, I-ORG, I-ORG, I-ORG, I-ORG]   \n",
       "19249                   [O, O, O, O, O, B-LOC, I-LOC, O]   \n",
       "5649                     [O, B-LOC, I-LOC, I-LOC, I-LOC]   \n",
       "11211                         [O, O, B-ORG, I-ORG, O, O]   \n",
       "18077                        [O, B-ORG, I-ORG, I-ORG, O]   \n",
       "12921              [O, O, B-PER, I-PER, O, B-PER, I-PER]   \n",
       "9350              [O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG]   \n",
       "12190                            [O, B-ORG, O, B-ORG, O]   \n",
       "12414  [B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG]   \n",
       "10482                       [B-LOC, I-LOC, I-LOC, I-LOC]   \n",
       "6427                        [B-LOC, I-LOC, I-LOC, I-LOC]   \n",
       "5138                            [O, B-PER, I-PER, I-PER]   \n",
       "14327                        [O, B-LOC, I-LOC, I-LOC, O]   \n",
       "1906                  [O, O, B-PER, I-PER, B-PER, I-PER]   \n",
       "7594                        [B-PER, I-PER, I-PER, I-PER]   \n",
       "\n",
       "                                        predicted_labels  \n",
       "16138                [B-ORG, I-ORG, I-ORG, I-ORG, I-ORG]  \n",
       "19249                           [O, O, O, O, O, O, O, O]  \n",
       "5649                     [O, B-LOC, B-LOC, I-LOC, I-LOC]  \n",
       "11211                         [O, O, I-PER, I-PER, O, O]  \n",
       "18077                    [O, B-PER, I-PER, I-PER, I-PER]  \n",
       "12921          [O, O, B-PER, I-PER, I-ORG, I-PER, I-PER]  \n",
       "9350              [O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG]  \n",
       "12190                        [O, I-PER, O, I-ORG, I-PER]  \n",
       "12414  [B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG]  \n",
       "10482                       [B-LOC, I-PER, I-PER, I-PER]  \n",
       "6427                        [B-LOC, I-LOC, B-LOC, I-LOC]  \n",
       "5138                            [O, B-LOC, B-PER, I-PER]  \n",
       "14327                        [O, B-LOC, O, I-LOC, B-LOC]  \n",
       "1906                  [O, O, B-PER, B-ORG, B-LOC, I-ORG]  \n",
       "7594                        [B-PER, I-PER, I-PER, I-PER]  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# First, let's group predictions by sample instead of having all tokens in one list\n",
    "def group_predictions_by_sample(predictions_list, test_dataset, id_2_class_map):\n",
    "    \"\"\"Group predictions by sample to match dataset structure\"\"\"\n",
    "    \n",
    "    sample_predictions = []\n",
    "    sample_idx = 0\n",
    "    \n",
    "    for batch_results in predictions_list:\n",
    "        pred_ids = batch_results['predictions']  # (batch_size, seq_len)\n",
    "        attention_masks = batch_results['attention_mask']  # (batch_size, seq_len)\n",
    "        \n",
    "        for i in range(pred_ids.size(0)):\n",
    "            if sample_idx >= len(test_dataset):\n",
    "                break\n",
    "                \n",
    "            # Get original sample to know how many tokens it has\n",
    "            original_idx = test_dataset.indices[sample_idx]\n",
    "            original_tokens = dataset[original_idx]['tokens']\n",
    "            \n",
    "            # Get predictions for this sequence\n",
    "            seq_predictions = pred_ids[i]\n",
    "            seq_mask = attention_masks[i]\n",
    "            \n",
    "            # Filter predictions by attention mask\n",
    "            valid_predictions = seq_predictions[seq_mask == 1]\n",
    "            pred_labels = [id_2_class_map.get(pred.item(), 'O') for pred in valid_predictions]\n",
    "            \n",
    "            # Truncate to match original token count\n",
    "            aligned_predictions = pred_labels[:len(original_tokens)]\n",
    "            \n",
    "            sample_predictions.append(aligned_predictions)\n",
    "            sample_idx += 1\n",
    "    \n",
    "    return sample_predictions\n",
    "\n",
    "# Group your predictions by sample\n",
    "print(\"Grouping predictions by sample...\")\n",
    "sample_predictions = group_predictions_by_sample(predictions_list, test_dataset, train_dataset.dataset.id_2_class)\n",
    "print(f\"Grouped into {len(sample_predictions)} samples\")\n",
    "\n",
    "# Get test indices\n",
    "test_indices = test_dataset.indices\n",
    "\n",
    "# Create DataFrame adapted for NER dataset\n",
    "df = pd.DataFrame(data={\n",
    "    \"tokens\": [dataset[idx]['tokens'] for idx in test_indices],\n",
    "    \"token_ids\": [ner_tokenizer(dataset[idx]['tokens'], is_split_into_words=True)['input_ids'] for idx in test_indices],\n",
    "    \"true_labels\": [[train_dataset.dataset.id_2_class[label] for label in dataset[idx]['ner_tags']] for idx in test_indices],\n",
    "    'predicted_labels': sample_predictions[:len(test_indices)]  # Use grouped predictions\n",
    "}, index=test_indices)\n",
    "\n",
    "# Add token strings (convert token IDs back to readable tokens)\n",
    "df['token_strings'] = df.token_ids.apply(lambda t: ner_tokenizer.convert_ids_to_tokens(t))\n",
    "\n",
    "# Reorder columns for better readability\n",
    "df = df[[\"tokens\", \"token_ids\", \"token_strings\", \"true_labels\", \"predicted_labels\"]]\n",
    "\n",
    "# Style the DataFrame for better display\n",
    "styled_df = df.style.set_table_styles(\n",
    "    [\n",
    "        {'selector': 'td', 'props': [('word-wrap', 'break-word')]}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"DataFrame created with {len(df)} samples\")\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_ids</th>\n",
       "      <th>token_strings</th>\n",
       "      <th>true_labels</th>\n",
       "      <th>predicted_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19249</th>\n",
       "      <td>[*El, primer, cuartel, simboliza, las, Islas, ...</td>\n",
       "      <td>[438, 3227, 2198, 990, 17310, 6294, 434, 1445,...</td>\n",
       "      <td>[Ġ*, El, Ġprimer, Ġcu, artel, Ġsim, bol, iza, ...</td>\n",
       "      <td>[O, O, O, O, O, B-LOC, I-LOC, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5649</th>\n",
       "      <td>[REDIRECCIÓN, Tubérculo, menor, del, húmero]</td>\n",
       "      <td>[276, 10777, 6464, 280, 360, 8957]</td>\n",
       "      <td>[ĠREDIRECCIÃĵN, ĠTubÃ©rculo, Ġmen, or, Ġdel, Ġ...</td>\n",
       "      <td>[O, B-LOC, I-LOC, I-LOC, I-LOC]</td>\n",
       "      <td>[O, B-LOC, B-LOC, I-LOC, I-LOC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11211</th>\n",
       "      <td>[', '', Coquimbo, Unido, '', ']</td>\n",
       "      <td>[257, 262, 4381, 1312, 262, 257]</td>\n",
       "      <td>[Ġ', Ġ'', ĠCoquimbo, ĠUnido, Ġ'', Ġ']</td>\n",
       "      <td>[O, O, B-ORG, I-ORG, O, O]</td>\n",
       "      <td>[O, O, I-PER, I-PER, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18077</th>\n",
       "      <td>[====, Circuitos, Continentales, UCI, ====]</td>\n",
       "      <td>[2051, 3184, 5132, 15376, 3048, 6286, 2051]</td>\n",
       "      <td>[Ġ====, ĠCircu, itos, ĠContin, entales, ĠUCI, ...</td>\n",
       "      <td>[O, B-ORG, I-ORG, I-ORG, O]</td>\n",
       "      <td>[O, B-PER, I-PER, I-PER, I-PER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12921</th>\n",
       "      <td>[junto, a, Ben, Chaplin, y, Idris, Elba]</td>\n",
       "      <td>[1009, 342, 839, 2623, 1979, 337, 6254, 865, 1...</td>\n",
       "      <td>[Ġjunto, Ġa, ĠBen, ĠChap, lin, Ġy, ĠId, ris, Ġ...</td>\n",
       "      <td>[O, O, B-PER, I-PER, O, B-PER, I-PER]</td>\n",
       "      <td>[O, O, B-PER, I-PER, I-ORG, I-PER, I-PER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12190</th>\n",
       "      <td>[===, Universal, /, PolyGram, ===]</td>\n",
       "      <td>[761, 4386, 499, 885, 89, 39, 4992, 761]</td>\n",
       "      <td>[Ġ===, ĠUniversal, Ġ/, ĠPol, y, G, ram, Ġ===]</td>\n",
       "      <td>[O, B-ORG, O, B-ORG, O]</td>\n",
       "      <td>[O, I-PER, O, I-ORG, I-PER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10482</th>\n",
       "      <td>[Santa, María, del, Oro]</td>\n",
       "      <td>[723, 547, 360, 1770]</td>\n",
       "      <td>[ĠSanta, ĠMarÃŃa, Ġdel, ĠOro]</td>\n",
       "      <td>[B-LOC, I-LOC, I-LOC, I-LOC]</td>\n",
       "      <td>[B-LOC, I-PER, I-PER, I-PER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6427</th>\n",
       "      <td>[Comarca, de, El, Morrazo]</td>\n",
       "      <td>[2646, 260, 437, 18436]</td>\n",
       "      <td>[ĠComarca, Ġde, ĠEl, ĠMorrazo]</td>\n",
       "      <td>[B-LOC, I-LOC, I-LOC, I-LOC]</td>\n",
       "      <td>[B-LOC, I-LOC, B-LOC, I-LOC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5138</th>\n",
       "      <td>[REDIRECCIÓN, Sebastián, Cirac, Estopañán]</td>\n",
       "      <td>[276, 2686, 18119, 6313, 19617]</td>\n",
       "      <td>[ĠREDIRECCIÃĵN, ĠSebastiÃ¡n, ĠCirac, ĠEsto, pa...</td>\n",
       "      <td>[O, B-PER, I-PER, I-PER]</td>\n",
       "      <td>[O, B-LOC, B-PER, I-PER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14327</th>\n",
       "      <td>[**, Perales, de, Tajuña, .]</td>\n",
       "      <td>[453, 605, 608, 260, 19388, 288]</td>\n",
       "      <td>[Ġ**, ĠPer, ales, Ġde, ĠTajuÃ±a, Ġ.]</td>\n",
       "      <td>[O, B-LOC, I-LOC, I-LOC, O]</td>\n",
       "      <td>[O, B-LOC, O, I-LOC, B-LOC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>[', '', Rennae, Stubbs, Helena, Suková]</td>\n",
       "      <td>[257, 262, 10137, 9268, 6241, 9006]</td>\n",
       "      <td>[Ġ', Ġ'', ĠRennae, ĠStubbs, ĠHelena, ĠSukovÃ¡]</td>\n",
       "      <td>[O, O, B-PER, I-PER, B-PER, I-PER]</td>\n",
       "      <td>[O, O, B-PER, B-ORG, B-LOC, I-ORG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13533</th>\n",
       "      <td>[Norman, Reedus, como, Daryl, Dixon, .]</td>\n",
       "      <td>[6680, 10670, 473, 4223, 1818, 6709, 288]</td>\n",
       "      <td>[ĠNorman, ĠReedus, Ġcomo, ĠDar, yl, ĠDixon, Ġ.]</td>\n",
       "      <td>[B-PER, I-PER, O, B-PER, I-PER, O]</td>\n",
       "      <td>[B-PER, B-PER, O, I-PER, O, I-PER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10138</th>\n",
       "      <td>[Escrita, por, Carlos, Orellana, .]</td>\n",
       "      <td>[3463, 326, 414, 714, 17248, 477, 288]</td>\n",
       "      <td>[ĠEscri, ta, Ġpor, ĠCarlos, ĠOrell, ana, Ġ.]</td>\n",
       "      <td>[O, O, B-PER, I-PER, O]</td>\n",
       "      <td>[B-PER, B-ORG, O, B-PER, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16915</th>\n",
       "      <td>[REDIRECCIÓN, Marismas, de, Pinsk]</td>\n",
       "      <td>[276, 5080, 2715, 260, 1564, 883]</td>\n",
       "      <td>[ĠREDIRECCIÃĵN, ĠMaris, mas, Ġde, ĠPin, sk]</td>\n",
       "      <td>[O, B-LOC, I-LOC, I-LOC]</td>\n",
       "      <td>[O, B-PER, I-PER, I-PER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640</th>\n",
       "      <td>[Anabel, Ochoa, (, 1955-2008, ), .]</td>\n",
       "      <td>[15697, 7397, 286, 16220, 13, 2859, 287, 288]</td>\n",
       "      <td>[ĠAnabel, ĠOchoa, Ġ(, Ġ1955, -, 2008, Ġ), Ġ.]</td>\n",
       "      <td>[B-PER, I-PER, O, O, O, O]</td>\n",
       "      <td>[O, B-PER, O, B-LOC, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tokens  \\\n",
       "19249  [*El, primer, cuartel, simboliza, las, Islas, ...   \n",
       "5649        [REDIRECCIÓN, Tubérculo, menor, del, húmero]   \n",
       "11211                    [', '', Coquimbo, Unido, '', ']   \n",
       "18077        [====, Circuitos, Continentales, UCI, ====]   \n",
       "12921           [junto, a, Ben, Chaplin, y, Idris, Elba]   \n",
       "12190                 [===, Universal, /, PolyGram, ===]   \n",
       "10482                           [Santa, María, del, Oro]   \n",
       "6427                          [Comarca, de, El, Morrazo]   \n",
       "5138          [REDIRECCIÓN, Sebastián, Cirac, Estopañán]   \n",
       "14327                       [**, Perales, de, Tajuña, .]   \n",
       "1906             [', '', Rennae, Stubbs, Helena, Suková]   \n",
       "13533            [Norman, Reedus, como, Daryl, Dixon, .]   \n",
       "10138                [Escrita, por, Carlos, Orellana, .]   \n",
       "16915                 [REDIRECCIÓN, Marismas, de, Pinsk]   \n",
       "2640                 [Anabel, Ochoa, (, 1955-2008, ), .]   \n",
       "\n",
       "                                               token_ids  \\\n",
       "19249  [438, 3227, 2198, 990, 17310, 6294, 434, 1445,...   \n",
       "5649                  [276, 10777, 6464, 280, 360, 8957]   \n",
       "11211                   [257, 262, 4381, 1312, 262, 257]   \n",
       "18077        [2051, 3184, 5132, 15376, 3048, 6286, 2051]   \n",
       "12921  [1009, 342, 839, 2623, 1979, 337, 6254, 865, 1...   \n",
       "12190           [761, 4386, 499, 885, 89, 39, 4992, 761]   \n",
       "10482                              [723, 547, 360, 1770]   \n",
       "6427                             [2646, 260, 437, 18436]   \n",
       "5138                     [276, 2686, 18119, 6313, 19617]   \n",
       "14327                   [453, 605, 608, 260, 19388, 288]   \n",
       "1906                 [257, 262, 10137, 9268, 6241, 9006]   \n",
       "13533          [6680, 10670, 473, 4223, 1818, 6709, 288]   \n",
       "10138             [3463, 326, 414, 714, 17248, 477, 288]   \n",
       "16915                  [276, 5080, 2715, 260, 1564, 883]   \n",
       "2640       [15697, 7397, 286, 16220, 13, 2859, 287, 288]   \n",
       "\n",
       "                                           token_strings  \\\n",
       "19249  [Ġ*, El, Ġprimer, Ġcu, artel, Ġsim, bol, iza, ...   \n",
       "5649   [ĠREDIRECCIÃĵN, ĠTubÃ©rculo, Ġmen, or, Ġdel, Ġ...   \n",
       "11211              [Ġ', Ġ'', ĠCoquimbo, ĠUnido, Ġ'', Ġ']   \n",
       "18077  [Ġ====, ĠCircu, itos, ĠContin, entales, ĠUCI, ...   \n",
       "12921  [Ġjunto, Ġa, ĠBen, ĠChap, lin, Ġy, ĠId, ris, Ġ...   \n",
       "12190      [Ġ===, ĠUniversal, Ġ/, ĠPol, y, G, ram, Ġ===]   \n",
       "10482                      [ĠSanta, ĠMarÃŃa, Ġdel, ĠOro]   \n",
       "6427                      [ĠComarca, Ġde, ĠEl, ĠMorrazo]   \n",
       "5138   [ĠREDIRECCIÃĵN, ĠSebastiÃ¡n, ĠCirac, ĠEsto, pa...   \n",
       "14327               [Ġ**, ĠPer, ales, Ġde, ĠTajuÃ±a, Ġ.]   \n",
       "1906      [Ġ', Ġ'', ĠRennae, ĠStubbs, ĠHelena, ĠSukovÃ¡]   \n",
       "13533    [ĠNorman, ĠReedus, Ġcomo, ĠDar, yl, ĠDixon, Ġ.]   \n",
       "10138       [ĠEscri, ta, Ġpor, ĠCarlos, ĠOrell, ana, Ġ.]   \n",
       "16915        [ĠREDIRECCIÃĵN, ĠMaris, mas, Ġde, ĠPin, sk]   \n",
       "2640       [ĠAnabel, ĠOchoa, Ġ(, Ġ1955, -, 2008, Ġ), Ġ.]   \n",
       "\n",
       "                                 true_labels  \\\n",
       "19249       [O, O, O, O, O, B-LOC, I-LOC, O]   \n",
       "5649         [O, B-LOC, I-LOC, I-LOC, I-LOC]   \n",
       "11211             [O, O, B-ORG, I-ORG, O, O]   \n",
       "18077            [O, B-ORG, I-ORG, I-ORG, O]   \n",
       "12921  [O, O, B-PER, I-PER, O, B-PER, I-PER]   \n",
       "12190                [O, B-ORG, O, B-ORG, O]   \n",
       "10482           [B-LOC, I-LOC, I-LOC, I-LOC]   \n",
       "6427            [B-LOC, I-LOC, I-LOC, I-LOC]   \n",
       "5138                [O, B-PER, I-PER, I-PER]   \n",
       "14327            [O, B-LOC, I-LOC, I-LOC, O]   \n",
       "1906      [O, O, B-PER, I-PER, B-PER, I-PER]   \n",
       "13533     [B-PER, I-PER, O, B-PER, I-PER, O]   \n",
       "10138                [O, O, B-PER, I-PER, O]   \n",
       "16915               [O, B-LOC, I-LOC, I-LOC]   \n",
       "2640              [B-PER, I-PER, O, O, O, O]   \n",
       "\n",
       "                                predicted_labels  \n",
       "19249                   [O, O, O, O, O, O, O, O]  \n",
       "5649             [O, B-LOC, B-LOC, I-LOC, I-LOC]  \n",
       "11211                 [O, O, I-PER, I-PER, O, O]  \n",
       "18077            [O, B-PER, I-PER, I-PER, I-PER]  \n",
       "12921  [O, O, B-PER, I-PER, I-ORG, I-PER, I-PER]  \n",
       "12190                [O, I-PER, O, I-ORG, I-PER]  \n",
       "10482               [B-LOC, I-PER, I-PER, I-PER]  \n",
       "6427                [B-LOC, I-LOC, B-LOC, I-LOC]  \n",
       "5138                    [O, B-LOC, B-PER, I-PER]  \n",
       "14327                [O, B-LOC, O, I-LOC, B-LOC]  \n",
       "1906          [O, O, B-PER, B-ORG, B-LOC, I-ORG]  \n",
       "13533         [B-PER, B-PER, O, I-PER, O, I-PER]  \n",
       "10138                [B-PER, B-ORG, O, B-PER, O]  \n",
       "16915                   [O, B-PER, I-PER, I-PER]  \n",
       "2640                  [O, B-PER, O, B-LOC, O, O]  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = df[df['true_labels'] != df['predicted_labels']]\n",
    "errors.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "- El performance del modelo es staisfactorio. La exerimentacion sugiere que usar mas epocas podria ser ideal para subir esas metricas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
